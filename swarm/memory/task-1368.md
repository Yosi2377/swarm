# Task: 1368 - ××—×§×¨ multi-agent systems

## ××©×™××”
××—×§×¨ ××§×™×£ ×¢×œ ××¢×¨×›×•×ª multi-agent: ××¡×¤×¨ ×¡×•×›× ×™× ××™×“×™××œ×™, ×ª×¤×§×™×“×™×, prompts, best practices, ×”×©×•×•××” ×œ××¢×¨×›×ª ×©×œ× ×•.

## ×××¦××™×

### 1. ×”×©×•×•××ª ×¤×¨×™×™××•×•×¨×§×™× ××•×‘×™×œ×™×

| Framework | Stars | Approach | # Agents | Roles |
|-----------|-------|----------|----------|-------|
| **CrewAI** | 27k+ | Role-based crews, task delegation | 2-7 per crew | Flexible (role/goal/backstory) |
| **AutoGen** (Microsoft) | 40k+ | Conversational agents, tool use | 2-10 | AssistantAgent, UserProxy, custom |
| **MetaGPT** | 48k+ | Software company SOP | 5-7 fixed | PM, Architect, Engineer, QA, Designer |
| **ChatDev** (OpenBMB) | 26k+ | Software company simulation | 4-6 fixed | CEO, CTO, Programmer, Reviewer, Tester |
| **LangGraph** | 10k+ | Graph-based state machine | Flexible | Node-based, any role |

### 2. ××¡×¤×¨ ×¡×•×›× ×™× ××™×“×™××œ×™

**Consensus from research & frameworks:**
- **3-5 agents** = sweet spot for most tasks
- **2 agents** = minimum for meaningful collaboration (e.g., coder + reviewer)
- **7+ agents** = diminishing returns, coordination overhead increases
- **MetaGPT paper** showed 5-7 specialized agents outperform single agents on software tasks
- **ChatDev research** (NeurIPS 2025) showed "evolving orchestration" with learnable coordinator improves results

**Key insight:** More agents â‰  better. The quality of role definitions and coordination protocol matters more than count.

### 3. ×ª×¤×§×™×“×™× × ×¤×•×¦×™× (Common Roles)

**Universal roles across frameworks:**
1. **Orchestrator/Manager** â€” ××ª××, ××—×œ×§ ××©×™××•×ª (CrewAI: Manager, MetaGPT: PM)
2. **Coder/Engineer** â€” ×›×•×ª×‘ ×§×•×“ (all frameworks)
3. **Reviewer/QA** â€” ×‘×•×“×§ ×§×•×“ ×•××™×›×•×ª (MetaGPT: QA, ChatDev: Reviewer)
4. **Researcher** â€” ××•×¡×£ ××™×“×¢ (CrewAI common pattern)
5. **Designer/Architect** â€” ××ª×›× ×Ÿ ××‘× ×” (MetaGPT: Architect)

**Specialized roles (when needed):**
- Security auditor
- Documentation writer
- DevOps/Deployment
- Data analyst

### 4. Best Practices for Agent Prompts

**From CrewAI docs (best documented):**
- **Role**: ××’×“×™×¨ ××ª ×”×¤×•× ×§×¦×™×” ×•×”××•××—×™×•×ª ("Senior Security Analyst")
- **Goal**: ××˜×¨×” ×¡×¤×¦×™×¤×™×ª ×©×× ×—×” ×”×—×œ×˜×•×ª ("Find and report all security vulnerabilities")
- **Backstory**: ×§×•× ×˜×§×¡×˜ ×•××™×©×™×•×ª ("You've been protecting systems for 20 years...")

**Key prompt patterns:**
1. **Be specific** â€” "Senior Python Developer" > "Developer"
2. **Give context** â€” backstory improves decision quality significantly
3. **Set boundaries** â€” what the agent should NOT do is as important as what it should
4. **Define output format** â€” clear expected deliverables
5. **Allow/deny delegation** â€” control who can ask others for help

**From AutoGen:**
- System message should be concise and focused
- Description field (separate from system_message) helps orchestrator route tasks
- Tools should be explicitly listed per agent

### 5. Coordination Patterns

| Pattern | Description | Used By | Best For |
|---------|-------------|---------|----------|
| **Sequential** | A â†’ B â†’ C pipeline | CrewAI, ChatDev | Linear workflows |
| **Hierarchical** | Manager delegates to workers | CrewAI, our system | Complex multi-domain tasks |
| **Round-Robin** | Agents take turns | AutoGen | Debate/review |
| **Graph/DAG** | State machine transitions | LangGraph | Complex conditional flows |
| **Autonomous** | Agents communicate freely | MetaGPT (SOP-guided) | Software development |

### 6. ××” ×”××¢×¨×›×ª ×©×œ× ×• ×¢×•×©×” × ×›×•×Ÿ âœ…

1. **Hierarchical orchestration** â€” orchestrator in General, agents in topics â† industry standard
2. **Clear role separation** â€” shomer, koder, tzayar, researcher, worker â† matches best practices
3. **Dedicated communication channels** â€” Agent Chat (479) â† similar to AutoGen group chat
4. **Task isolation** â€” each task in its own topic â† good pattern
5. **Quality gates** â€” dual review (code + UX) â† better than most frameworks
6. **Persistent memory** â€” task files, vault â† essential for long-running work
7. **Bot identities** â€” visible agent identity in Telegram â† unique advantage

### 7. ××” ×—×¡×¨ / ××” ×œ×©×¤×¨ ğŸ”§

1. **Auto-routing logic** â€” currently manual orchestration. CrewAI/LangGraph do automatic task routing based on agent descriptions
2. **Agent descriptions** â€” our agents lack a `description` field (separate from role). AutoGen uses this for routing
3. **Feedback loops** â€” no automatic retry/iteration between agents. MetaGPT has built-in review cycles
4. **Shared context/memory** â€” agents don't easily share findings. CrewAI has shared memory across crew
5. **Parallel execution** â€” our system is mostly sequential. LangGraph supports parallel branches
6. **Agent count** â€” 5 agents is within sweet spot (3-7), but consider:
   - Merging worker + researcher (overlap in general tasks)
   - Adding a dedicated **DevOps/Deployer** if needed
7. **Prompt structure** â€” our agent prompts could benefit from:
   - Explicit `description` (one-liner for routing)
   - `backstory` (personality/context)
   - `constraints` (what NOT to do)
8. **Tooling per agent** â€” no tool restrictions per agent. CrewAI assigns specific tools per agent

### 8. ×”××œ×¦×•×ª ××¡×›××•×ª

1. **Keep 5 agents** â€” it's the sweet spot. Don't add more unless specific need
2. **Enhance agent definitions** â€” add description, backstory, constraints to each agent in agents.json
3. **Add auto-routing** â€” orchestrator should match tasks to agents based on keywords + descriptions
4. **Implement feedback loops** â€” if koder's code fails review, auto-reassign back (partially done with 3-strike rule)
5. **Shared memory layer** â€” let agents read each other's task findings easily
6. **Consider merging** worker â†” researcher (both handle general tasks)
7. **Add agent metrics** â€” track success rate, average task time per agent
